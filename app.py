import streamlit as st
import pandas as pd
import numpy as np
import json
import re
import sys
import os
import time
import warnings
import html
from urllib.parse import urlparse, quote, unquote
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from bs4 import BeautifulSoup
from dateutil.parser import parse as parse_date
from datetime import datetime, timedelta, timezone
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_score
from pyvi import ViTokenizer
from stopwordsiso import stopwords
import feedparser
import google.generativeai as genai
from dotenv import load_dotenv
import requests
# from streamlit_extras.app_logo import app_logo
# --- C·∫§U H√åNH TRANG V√Ä CSS ---

st.set_page_config(page_title="T·∫°p ch√≠ c·ªßa b·∫°n", page_icon="üìñ", layout="wide")

# Load environment variables
load_dotenv()

# C√°c ngu·ªìn RSS ƒë·ªÉ thu th·∫≠p d·ªØ li·ªáu
RSS_URLS = [
    "https://dantri.com.vn/rss/home.rss",
    "https://dantri.com.vn/rss/xa-hoi.rss",
    "https://dantri.com.vn/rss/gia-vang.rss",
    "https://dantri.com.vn/rss/the-thao.rss",
    "https://dantri.com.vn/rss/giao-duc.rss",
    "https://dantri.com.vn/rss/kinh-doanh.rss",
    "https://dantri.com.vn/rss/giai-tri.rss",
    "https://dantri.com.vn/rss/phap-luat.rss",
    "https://dantri.com.vn/rss/cong-nghe.rss",
    "https://dantri.com.vn/rss/tinh-yeu-gioi-tinh.rss",
    "https://dantri.com.vn/rss/noi-vu.rss",
    "https://dantri.com.vn/rss/tam-diem.rss",
    "https://dantri.com.vn/rss/infographic.rss",
    "https://dantri.com.vn/rss/dnews.rss",
    "https://dantri.com.vn/rss/xo-so.rss",
    "https://dantri.com.vn/rss/tet-2025.rss",
    "https://dantri.com.vn/rss/d-buzz.rss",
    "https://dantri.com.vn/rss/su-kien.rss",
    "https://dantri.com.vn/rss/the-gioi.rss",
    "https://dantri.com.vn/rss/doi-song.rss",
    "https://dantri.com.vn/rss/lao-dong-viec-lam.rss",
    "https://dantri.com.vn/rss/tam-long-nhan-ai.rss",
    "https://dantri.com.vn/rss/bat-dong-san.rss",
    "https://dantri.com.vn/rss/du-lich.rss",
    "https://dantri.com.vn/rss/suc-khoe.rss",
    "https://dantri.com.vn/rss/o-to-xe-may.rss",
    "https://dantri.com.vn/rss/khoa-hoc.rss",
    "https://dantri.com.vn/rss/ban-doc.rss",
    "https://dantri.com.vn/rss/dmagazine.rss",
    "https://dantri.com.vn/rss/photo-news.rss",
    "https://dantri.com.vn/rss/toa-dam-truc-tuyen.rss",
    "https://dantri.com.vn/rss/interactive.rss",
    "https://dantri.com.vn/rss/photo-story.rss",
    "https://vnexpress.net/rss/tin-moi-nhat.rss",      # Trang ch·ªß (th∆∞·ªùng l√† tin m·ªõi nh·∫•t)
    "https://vnexpress.net/rss/the-gioi.rss",
    "https://vnexpress.net/rss/thoi-su.rss",
    "https://vnexpress.net/rss/kinh-doanh.rss",
    "https://vnexpress.net/rss/startup.rss",
    "https://vnexpress.net/rss/giai-tri.rss",
    "https://vnexpress.net/rss/the-thao.rss",
    "https://vnexpress.net/rss/phap-luat.rss",
    "https://vnexpress.net/rss/giao-duc.rss",
    "https://vnexpress.net/rss/tin-noi-bat.rss",
    
    # # C·ªôt b√™n ph·∫£i
    "https://vnexpress.net/rss/suc-khoe.rss",
    "https://vnexpress.net/rss/doi-song.rss",
    "https://vnexpress.net/rss/du-lich.rss",
    "https://vnexpress.net/rss/khoa-hoc.rss",         # Khoa h·ªçc c√¥ng ngh·ªá
    "https://vnexpress.net/rss/xe.rss",
    "https://vnexpress.net/rss/y-kien.rss",
    "https://vnexpress.net/rss/tam-su.rss",
    "https://vnexpress.net/rss/cuoi.rss",
    "https://vnexpress.net/rss/tin-xem-nhieu.rss",
    "https://thanhnien.vn/rss/home.rss",                   # Trang ch·ªß
    "https://thanhnien.vn/rss/thoi-su.rss",
    "https://thanhnien.vn/rss/chinh-tri.rss",
    "https://thanhnien.vn/rss/chao-ngay-moi.rss",
    "https://thanhnien.vn/rss/the-gioi.rss",
    "https://thanhnien.vn/rss/kinh-te.rss",
    "https://thanhnien.vn/rss/doi-song.rss",
    "https://thanhnien.vn/rss/suc-khoe.rss",
    "https://thanhnien.vn/rss/gioi-tre.rss",
    "https://thanhnien.vn/rss/tieu-dung-thong-minh.rss",
    "https://thanhnien.vn/rss/giao-duc.rss",
    "https://thanhnien.vn/rss/du-lich.rss",
    "https://thanhnien.vn/rss/van-hoa.rss",
    "https://thanhnien.vn/rss/giai-tri.rss",
    "https://thanhnien.vn/rss/the-thao.rss",
    "https://thanhnien.vn/rss/cong-nghe.rss",
    "https://thanhnien.vn/rss/xe.rss",
    "https://thanhnien.vn/rss/thoi-trang-tre.rss",
    "https://thanhnien.vn/rss/ban-doc.rss",
    "https://thanhnien.vn/rss/rao-vat.rss",
    "https://thanhnien.vn/rss/video.rss",
    "https://thanhnien.vn/rss/dien-dan.rss",
    "https://thanhnien.vn/rss/podcast.rss",
    "https://thanhnien.vn/rss/nhat-ky-tet-viet.rss",
    "https://thanhnien.vn/rss/magazine.rss",
    "https://thanhnien.vn/rss/cung-con-di-tiep-cuoc-doi.rss",
    "https://thanhnien.vn/rss/ban-can-biet.rss",
    "https://thanhnien.vn/rss/cai-chinh.rss",
    "https://thanhnien.vn/rss/blog-phong-vien.rss",
    "https://thanhnien.vn/rss/toi-viet.rss",
    "https://thanhnien.vn/rss/viec-lam.rss",
    "https://thanhnien.vn/rss/tno.rss",
    "https://thanhnien.vn/rss/tin-24h.rss",
    "https://thanhnien.vn/rss/thi-truong.rss",
    "https://thanhnien.vn/rss/tin-nhanh-360.tno",
     # C·ªôt b√™n tr√°i
    "https://tuoitre.vn/rss/tin-moi-nhat.rss",  # Trang ch·ªß
    "https://tuoitre.vn/rss/the-gioi.rss",
    "https://tuoitre.vn/rss/kinh-doanh.rss",
    "https://tuoitre.vn/rss/xe.rss",
    "https://tuoitre.vn/rss/van-hoa.rss",
    "https://tuoitre.vn/rss/the-thao.rss",
    "https://tuoitre.vn/rss/khoa-hoc.rss",
    "https://tuoitre.vn/rss/gia-that.rss",
    "https://tuoitre.vn/rss/ban-doc.rss",
    "https://tuoitre.vn/rss/video.rss",

    # C·ªôt b√™n ph·∫£i
    "https://tuoitre.vn/rss/thoi-su.rss",
    "https://tuoitre.vn/rss/phap-luat.rss",
    "https://tuoitre.vn/rss/cong-nghe.rss",
    "https://tuoitre.vn/rss/nhip-song-tre.rss",
    "https://tuoitre.vn/rss/giai-tri.rss",
    "https://tuoitre.vn/rss/giao-duc.rss",
    "https://tuoitre.vn/rss/suc-khoe.rss",
    "https://tuoitre.vn/rss/thu-gian.rss",
    "https://tuoitre.vn/rss/du-lich.rss"
]

# Initialize session state
if 'read_articles' not in st.session_state:
    st.session_state.read_articles = set()
if 'reading_history' not in st.session_state:
    st.session_state.reading_history = []
if 'current_view' not in st.session_state:
    st.session_state.current_view = "main"
if 'current_article_id' not in st.session_state:
    st.session_state.current_article_id = None
if 'selected_topic' not in st.session_state:
    st.session_state.selected_topic = "D√†nh cho b·∫°n (T·∫•t c·∫£)"
if 'selected_sources' not in st.session_state:
    st.session_state.selected_sources = []
if 'update_log' not in st.session_state:
    st.session_state.update_log = ""
if 'update_error' not in st.session_state:
    st.session_state.update_error = ""
if 'update_success' not in st.session_state:
    st.session_state.update_success = False
if 'is_processing' not in st.session_state:
    st.session_state.is_processing = False
if 'processing_progress' not in st.session_state:
    st.session_state.processing_progress = {
        'step': '',
        'message': '',
        'progress': 0
    }

@st.cache_resource
def get_sbert_model():
    return SentenceTransformer('Cloyne/vietnamese-sbert-v3')

def local_css(file_name):
    try:
        with open(file_name, "r", encoding="utf-8") as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
    except FileNotFoundError:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{file_name}'.")

# --- C√ÅC H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU ---
def get_source_name(link):
    try:
        domain = urlparse(link).netloc
        if domain.startswith('www.'): domain = domain[4:]
        return domain.split('.')[0].capitalize()
    except:
        return "N/A"

def normalize_title(title):
    return html.unescape(title).strip().lower()

@st.cache_data(ttl=3600)  # Cache for 1 hour
def fetch_recent_articles(rss_urls, hours=24):
    """Fetch recent articles from RSS feeds."""
    articles = []
    seen_titles = set()
    seen_links = set()
    time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours)
    
    for url in rss_urls:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            norm_title = normalize_title(entry.title)
            link = entry.link.strip()
            if norm_title in seen_titles or link in seen_links:
                continue
            published_time = entry.get("published", "")
            summary_raw = entry.get("summary", "")
            image_url = None
            if summary_raw:
                soup = BeautifulSoup(summary_raw, 'html.parser')
                img_tag = soup.find('img')
                if img_tag and 'src' in img_tag.attrs:
                    image_url = img_tag['src']
            source_name = get_source_name(entry.link)
            if published_time:
                try:
                    parsed_time = parse_date(published_time).astimezone(timezone.utc)
                    if parsed_time >= time_threshold:
                        articles.append({
                            "title": entry.title,
                            "link": entry.link,
                            "summary_raw": summary_raw,
                            "published_time": parsed_time.isoformat(),
                            "image_url": image_url,
                            "source": source_name,
                            "source_name": source_name  # Th√™m c·ªôt source_name
                        })
                        seen_titles.add(norm_title)
                        seen_links.add(link)
                except (ValueError, TypeError):
                    continue
    return pd.DataFrame(articles)

@st.cache_data(ttl=3600)
def clean_text(df):
    """Clean and process text from articles."""
    # L∆∞u l·∫°i c√°c c·ªôt quan tr·ªçng
    important_columns = ['title', 'link', 'published_time', 'image_url', 'source', 'source_name']
    
    # X·ª≠ l√Ω vƒÉn b·∫£n
    summary = (df['summary_raw']
               .str.lower()
               .str.replace(r'<.*?>', '', regex=True)
               .str.replace(r'[^\w\s]', '', regex=True))
    df['summary_cleaned'] = summary
    df.dropna(subset=['summary_cleaned'], inplace=True)
    df = df[df['summary_cleaned'].str.strip() != '']
    df = df[df['summary_cleaned'].str.split().str.len() >= 10]
    
    vi_stop = stopwords("vi")
    def remove_stop_pyvi(text):
        tokens = ViTokenizer.tokenize(text).split()
        filtered = [t for t in tokens if t not in vi_stop]
        return " ".join(filtered)
    
    df['summary_not_stop_word'] = df['summary_cleaned'].apply(remove_stop_pyvi)
    
    # ƒê·∫£m b·∫£o c√°c c·ªôt quan tr·ªçng v·∫´n c√≤n trong DataFrame
    for col in important_columns:
        if col not in df.columns:
            df[col] = df[col] if col in df.columns else None
            
    return df.reset_index(drop=True)

@st.cache_data(ttl=3600)
def vectorize_text(sentences, _model):
    """Vectorize text using S-BERT."""
    return _model.encode(sentences, show_progress_bar=True)

@st.cache_data(ttl=3600)
def generate_meaningful_topic_name(keywords, sample_titles):
    """Generate topic name using Gemini."""
    try:
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω bi√™n t·∫≠p b√°o ch√≠. D·ª±a v√†o c√°c th√¥ng tin d∆∞·ªõi ƒë√¢y, h√£y t·∫°o ra ch·ªâ duy nh·∫•t m·ªôt t√™n ch·ªß ƒë·ªÅ ng·∫Øn g·ªçn (kh√¥ng qu√° 6 t·ª´, kh√¥ng c·∫ßn di·ªÖn gi·∫£i) b·∫±ng ti·∫øng Vi·ªát ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung ch√≠nh.
        C√°c t·ª´ kh√≥a ch√≠nh c·ªßa ch·ªß ƒë·ªÅ: {keywords}
        M·ªôt v√†i ti√™u ƒë·ªÅ b√†i vi·∫øt v√≠ d·ª•:
        - {"\n- ".join(sample_titles)}
        T√™n ch·ªß ƒë·ªÅ g·ª£i √Ω:"""
        response = model.generate_content(prompt)
        return response.text.strip().replace("*", "")
    except Exception as e:
        return keywords

@st.cache_data(ttl=3600)
def get_topic_labels(df, num_keywords=5):
    """Generate topic labels for clusters."""
    topic_labels = {}
    actual_clusters = df['topic_cluster'].nunique()
    for i in range(actual_clusters):
        cluster_df = df[df['topic_cluster'] == i]
        cluster_texts = cluster_df['summary_cleaned'].tolist()
        if len(cluster_texts) < 3:
            topic_labels[str(i)] = "Ch·ªß ƒë·ªÅ nh·ªè (√≠t b√†i vi·∫øt)"
            continue
        vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        tfidf_matrix = vectorizer.fit_transform(cluster_texts)
        avg_tfidf_scores = tfidf_matrix.mean(axis=0).A1
        top_indices = avg_tfidf_scores.argsort()[-num_keywords:][::-1]
        feature_names = vectorizer.get_feature_names_out()
        keywords = ", ".join([feature_names[j] for j in top_indices])
        sample_titles = cluster_df['title'].head(3).tolist()
        meaningful_name = generate_meaningful_topic_name(keywords, sample_titles)
        topic_labels[str(i)] = meaningful_name
    return topic_labels

@st.cache_data(ttl=3600)
def process_articles():
    """Main function to process articles and return all necessary data."""
    # Fetch and clean articles
    df = fetch_recent_articles(RSS_URLS, hours=24)
    if df.empty:
        return None, None, None, None
    
    df = clean_text(df)
    
    # Get SBERT model and vectorize
    sbert_model = get_sbert_model()
    embeddings = vectorize_text(df['summary_not_stop_word'].tolist(), _model=sbert_model)
    
    # Find optimal number of clusters
    silhouette_scores = []
    possible_k_values = range(2, 20)
    for k in possible_k_values:
        kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans_temp.fit_predict(embeddings)
        score = silhouette_score(embeddings, labels)
        silhouette_scores.append(score)
    
    best_k = possible_k_values[np.argmax(silhouette_scores)]
    
    # Perform final clustering
    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    df['topic_cluster'] = kmeans.fit_predict(embeddings)
    
    # Generate topic labels
    topic_labels = get_topic_labels(df)
    
    # Calculate similarity matrix
    cosine_sim = cosine_similarity(embeddings)
    
    return df, cosine_sim, topic_labels, embeddings

# --- H√ÄM HI·ªÇN TH·ªä (RENDER) ---
def render_main_grid(df, selected_topic_name):
    st.header(f"B·∫£ng tin: {selected_topic_name}")
    st.markdown(f"T√¨m th·∫•y **{len(df)}** b√†i vi·∫øt li√™n quan.")
    st.markdown("---")
    num_columns = 3
    cols = st.columns(num_columns)
    if df.empty:
        st.warning("Kh√¥ng c√≥ b√†i vi·∫øt n√†o ph√π h·ª£p v·ªõi l·ª±a ch·ªçn c·ªßa b·∫°n.")
    else:
        for i, (index, row) in enumerate(df.iterrows()):
            with cols[i % num_columns]:
                # X·ª≠ l√Ω h√¨nh ·∫£nh v·ªõi placeholder
                image_html = ''
                if pd.notna(row["image_url"]):
                    image_html = f'<div class="card-image-container"><img src="{row["image_url"]}" onerror="this.onerror=null; this.src=\'no-image-png-2.webp\';"></div>'
                else:
                    image_html = '<div class="card-image-container"><img src="no-image-png-2.webp"></div>'
                
                # S·ª≠ d·ª•ng c·ªôt 'source_name' ƒë√£ t·∫°o
                source_name = row['source_name']


                card_html = f"""<div class="article-card">
                                {image_html}
                                <div class="article-content">
                                    <div class="article-title">{row['title']}</div>
                                    <div class="article-source">{source_name}</div>
                                </div>
                           </div>"""
                st.markdown(card_html, unsafe_allow_html=True)
                if st.button("ƒê·ªçc b√†i vi·∫øt", key=f"read_{index}"):
                    st.session_state.current_article_id = index
                    st.session_state.current_view = "detail"
                    st.rerun()

                

def calculate_interest_vector(df, cosine_sim, article_ids):
    """Calculate interest vector from reading history."""
    if not article_ids:
        return None
    
    # Get vectors for articles in history
    vectors = []
    for article_id in article_ids:
        if article_id < len(cosine_sim):
            vectors.append(cosine_sim[article_id])
    
    if not vectors:
        return None
    
    # Calculate average vector
    avg_vector = np.mean(vectors, axis=0)
    return avg_vector

def update_interest_vector(df, cosine_sim, article_id):
    """Update interest vector when new article is read."""
    if article_id not in st.session_state.reading_history:
        # Add to reading history (keep last 5)
        st.session_state.reading_history.insert(0, article_id)
        st.session_state.reading_history = st.session_state.reading_history[:5]
        
        # Calculate new interest vector
        st.session_state.interest_vector = calculate_interest_vector(
            df, cosine_sim, st.session_state.reading_history
        )
        
        # Update interest articles if vector exists
        if st.session_state.interest_vector is not None:
            similarity_scores = cosine_similarity([st.session_state.interest_vector], cosine_sim)[0]
            # Create mask to exclude read articles
            mask = ~df.index.isin(st.session_state.reading_history)
            similarity_scores[~mask] = -1  # Set similarity to -1 for read articles
            # Get top similar articles
            top_indices = np.argsort(similarity_scores)[::-1][:10]
            st.session_state.interest_articles = df.iloc[top_indices].copy()
            st.session_state.interest_articles['similarity_score'] = similarity_scores[top_indices]

def get_interest_articles():
    """Get articles based on user's interests."""
    if st.session_state.interest_articles is not None:
        return st.session_state.interest_articles
    return pd.DataFrame()

def calculate_average_vector(article_ids, cosine_sim):
    """Calculate average vector from last 5 articles."""
    if not article_ids:
        return None
    
    vectors = []
    for article_id in article_ids:
        if article_id < len(cosine_sim):
            vectors.append(cosine_sim[article_id])
    
    if not vectors:
        return None
    
    return np.mean(vectors, axis=0)

def get_similar_articles_by_history(df, cosine_sim, history_articles, exclude_articles=None):
    """Get similar articles based on reading history."""
    if not history_articles:
        return pd.DataFrame()
    
    # Ch·ªâ l·∫•y 5 b√†i vi·∫øt m·ªõi ƒë·ªçc g·∫ßn nh·∫•t
    recent_articles = history_articles[:5]
    
    avg_vector = calculate_average_vector(recent_articles, cosine_sim)
    if avg_vector is None:
        return pd.DataFrame()
    
    # Calculate similarity scores
    similarity_scores = cosine_similarity([avg_vector], cosine_sim)[0]
    
    # Create mask to exclude read articles
    if exclude_articles:
        mask = ~df.index.isin(exclude_articles)
        similarity_scores[~mask] = -1
    
    # Get top similar articles
    top_indices = np.argsort(similarity_scores)[::-1][:10]
    similar_articles = df.iloc[top_indices].copy()
    similar_articles['similarity_score'] = similarity_scores[top_indices]
    
    return similar_articles

def crawl_article_content(url):
    """Crawl n·ªôi dung b√†i vi·∫øt t·ª´ URL."""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        # X·ª≠ l√Ω lazy load ·∫£nh: chuy·ªÉn data-src th√†nh src
        for img in soup.find_all('img'):
            if img.has_attr('data-src'):
                img['src'] = img['data-src']
            # X√≥a img kh√¥ng c√≥ src ho·∫∑c src r·ªóng
            if not img.has_attr('src') or not img['src'].strip():
                img.decompose()

        # X√≥a c√°c figure r·ªóng ho·∫∑c ch·ªâ ch·ª©a ·∫£nh l·ªói
        for fig in soup.find_all('figure'):
            if not fig.text.strip() and not fig.find('img'):
                fig.decompose()

        # VnExpress
        if 'vnexpress.net' in url:
            article = soup.find('article', class_='fck_detail')
            if not article:
                # fallback: l·∫•y article ƒë·∫ßu ti√™n
                article = soup.find('article')
            if article:
                # X√≥a c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
                for tag in article.find_all(['script', 'style', 'iframe']):
                    tag.decompose()
                return str(article)

        # Tu·ªïi Tr·∫ª, Thanh Ni√™n
        elif 'tuoitre.vn' in url or 'thanhnien.vn' in url:
            article = soup.find('div', class_='detail-content')
            if not article:
                # fallback: l·∫•y div l·ªõn nh·∫•t
                divs = soup.find_all('div')
                article = max(divs, key=lambda d: len(d.text)) if divs else None
            if article:
                for tag in article.find_all(['script', 'style', 'iframe']):
                    tag.decompose()
                return str(article)

        # D√¢n tr√≠
        elif 'dantri.com.vn' in url:
            article = soup.find('div', class_='dt-news__content')
            if not article:
                # fallback: l·∫•y article ƒë·∫ßu ti√™n
                article = soup.find('article')
            if not article:
                # fallback: l·∫•y div l·ªõn nh·∫•t
                divs = soup.find_all('div')
                article = max(divs, key=lambda d: len(d.text)) if divs else None
            if article:
                for tag in article.find_all(['script', 'style', 'iframe']):
                    tag.decompose()
                return str(article)

        return None
    except Exception as e:
        print(f"L·ªói khi crawl b√†i vi·∫øt: {str(e)}")
        return None

def render_detail_view(article_id, df, cosine_sim, topic_labels):
    try:
        article = df.loc[article_id]
    except KeyError:
        st.error("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt.")
        if st.button("‚¨ÖÔ∏è Quay l·∫°i danh s√°ch"):
            st.session_state.current_view = "main"
            st.session_state.current_article_id = None
            st.rerun()
        return
    
    # Add article to read articles set and update reading history
    st.session_state.read_articles.add(article_id)
    if article_id not in st.session_state.reading_history:
        st.session_state.reading_history.insert(0, article_id)
    
    # ·∫®n thanh b√™n v√† ·∫£nh trong n·ªôi dung b√†i vi·∫øt
    st.markdown("""
        <style>
            [data-testid="stSidebar"][aria-expanded="true"]{
                display: none;
            }
            /* C·ª° ch·ªØ cho to√†n b·ªô n·ªôi dung article */
            article {
                font-size: 10px !important;
                line-height: 1.7 !important;
                margin: 0 !important;
                padding: 0 !important;
            }
            /* C·ª° ch·ªØ cho c√°c class detail */
            .article-content, .fck_detail, .detail-content, .dt-news__content {
                font-size: 15px !important;
                line-height: 1.6 !important;
                margin: 0 !important;
                padding: 0 !important;
            }
            .article-content img,
            .fck_detail img,
            .detail-content img,
            .dt-news__content img {
                max-width: 800px !important;
                height: auto !important;
                display: block;
                margin: 10px auto 10px auto;
                border-radius: 8px;
                box-shadow: 0 1px 4px rgba(0,0,0,0.08);
            }
            .recommendation-image {
                width: 240px !important;
                height: 140px !important;
                object-fit: cover !important;
                border-radius: 6px !important;
                box-shadow: 0 1px 2px rgba(0,0,0,0.08) !important;
                margin-right: 8px !important;
                display: block;
            }
            .block-container .stColumns {
                gap: 0 !important;
            }
            .recommendation-caption {
                font-size: 11px !important;
                color: #888;
                margin-top: 2px;
                white-space: nowrap;
                overflow: hidden;
                text-overflow: ellipsis;
                max-width: 100%;
                display: block;
            }
        </style>
    """, unsafe_allow_html=True)
    
    # N√∫t quay l·∫°i
    if st.button("‚¨ÖÔ∏è Quay l·∫°i danh s√°ch"):
        st.session_state.current_view = "main"
        st.session_state.current_article_id = None
        st.rerun()
    
    st.title(article['title'])
    
    # Chuy·ªÉn ƒë·ªïi th·ªùi gian ƒëƒÉng b√†i
    try:
        published_time = pd.to_datetime(article['published_time'])
        vn_time = published_time.tz_convert('Asia/Ho_Chi_Minh')
        time_str = vn_time.strftime('%d-%m-%Y %H:%M')
    except:
        time_str = article['published_time']
    
    st.caption(f"Ngu·ªìn: {article['source_name']} | Xu·∫•t b·∫£n: {time_str}")
    st.markdown("---")
    
    # Crawl v√† hi·ªÉn th·ªã n·ªôi dung b√†i vi·∫øt
    with st.spinner("ƒêang t·∫£i n·ªôi dung b√†i vi·∫øt..."):
        article_content = crawl_article_content(article['link'])
        
    if article_content:
        # Hi·ªÉn th·ªã n·ªôi dung b√†i vi·∫øt
        st.markdown(article_content, unsafe_allow_html=True)
        
        # Th√™m CSS ƒë·ªÉ ƒë·ªãnh d·∫°ng n·ªôi dung
        st.markdown("""
            <style>
                .article-content p {
                    font-size: 10px;
                    line-height: 1.6;
                    margin: 10px 0;
                }
                .article-content h1, .article-content h2, .article-content h3 {
                    margin: 20px 0 10px 0;
                }
            </style>
        """, unsafe_allow_html=True)
    else:
        # N·∫øu kh√¥ng crawl ƒë∆∞·ª£c, hi·ªÉn th·ªã t√≥m t·∫Øt
        st.subheader("T√≥m t·∫Øt")
        summary_raw = article.get('summary_raw', '')
        summary_without_img = re.sub(r'<img[^>]*>', '', summary_raw, flags=re.IGNORECASE)
        st.markdown(summary_without_img, unsafe_allow_html=True)
    
    # N√∫t ƒë·ªçc b√†i vi·∫øt g·ªëc
    st.link_button("ƒê·ªçc to√†n b·ªô b√†i vi·∫øt tr√™n trang g·ªëc", article['link'])
    
    # Ph·∫ßn b√†i vi·∫øt li√™n quan
    st.markdown("---")
    st.subheader("Kh√°m ph√° th√™m")
    rec_type = st.radio("Hi·ªÉn th·ªã c√°c b√†i vi·∫øt:", ("C√≥ n·ªôi dung t∆∞∆°ng t·ª±", "Trong c√πng ch·ªß ƒë·ªÅ"), key=f"rec_type_{article_id}")
    
    if rec_type == "C√≥ n·ªôi dung t∆∞∆°ng t·ª±":
        st.markdown("##### D·ª±a tr√™n ph√¢n t√≠ch ng·ªØ nghƒ©a:")
        sim_scores = sorted(list(enumerate(cosine_sim[article_id])), key=lambda x: x[1], reverse=True)[1:6]
        for i, (article_index, score) in enumerate(sim_scores):
            rec_article = df.iloc[article_index]
            with st.container(border=True):
                rec_col1, rec_col2 = st.columns([0.13, 0.87])  # ·∫¢nh nh·ªè h∆°n, ch·ªØ r·ªông h∆°n
                with rec_col1:
                    if pd.notna(rec_article['image_url']):
                        st.markdown(f'<img src="{rec_article["image_url"]}" onerror="this.onerror=null; this.src=\'no-image-png-2.webp\';" class="recommendation-image">', unsafe_allow_html=True)
                    else:
                        st.markdown('<img src="no-image-png-2.webp" class="recommendation-image">', unsafe_allow_html=True)
                with rec_col2:
                    if st.button(rec_article['title'], key=f"rec_{article_index}"):
                        st.session_state.current_article_id = article_index
                        st.rerun()
                    st.markdown(f'<div class="recommendation-caption">Ngu·ªìn: {rec_article["source_name"]} | ƒê·ªô t∆∞∆°ng ƒë·ªìng: {score:.2f}</div>', unsafe_allow_html=True)
    else:
        cluster_id = article['topic_cluster']
        topic_name = topic_labels.get(str(cluster_id), "N/A")
        st.markdown(f"##### Thu·ªôc ch·ªß ƒë·ªÅ: **{topic_name}**")
        same_cluster_df = df[(df['topic_cluster'] == cluster_id) & (df.index != article_id)].head(5)
        for i, row in same_cluster_df.iterrows():
            with st.container(border=True):
                rec_col1, rec_col2 = st.columns([0.13, 0.87])  # ·∫¢nh nh·ªè h∆°n, ch·ªØ r·ªông h∆°n
                with rec_col1:
                    if pd.notna(row['image_url']):
                        st.markdown(f'<img src="{row["image_url"]}" onerror="this.onerror=null; this.src=\'no-image-png-2.webp\';" class="recommendation-image">', unsafe_allow_html=True)
                    else:
                        st.markdown('<img src="no-image-png-2.webp" class="recommendation-image">', unsafe_allow_html=True)
                with rec_col2:
                    if st.button(row['title'], key=f"rec_{i}"):
                        st.session_state.current_article_id = i
                        st.rerun()
                    similarity_score = cosine_sim[article_id][i]
                    st.markdown(f'<div class="recommendation-caption">Ngu·ªìn: {row["source_name"]} | ƒê·ªô t∆∞∆°ng ƒë·ªìng: {similarity_score:.2f}</div>', unsafe_allow_html=True)

def render_search_results(query, df, embeddings, sbert_model):
    """Vector h√≥a truy v·∫•n v√† hi·ªÉn th·ªã k·∫øt qu·∫£ t√¨m ki·∫øm."""
    st.header(f"K·∫øt qu·∫£ t√¨m ki·∫øm cho: \"{query}\"")
    # Vector h√≥a c√¢u truy v·∫•n
    with st.spinner("ƒêang ph√¢n t√≠ch v√† t√¨m ki·∫øm..."):
        query_vector = sbert_model.encode([query])
        # T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng
        similarities = cosine_similarity(query_vector, embeddings)[0]
        # S·∫Øp x·∫øp v√† l·∫•y k·∫øt qu·∫£
        sim_scores = sorted(list(enumerate(similarities)), key=lambda x: x[1], reverse=True)
        result_indices = [i[0] for i in sim_scores]
        result_df = df.iloc[result_indices].copy()
    render_main_grid(result_df, f"K·∫øt qu·∫£ cho: \"{query}\"")

# --- LU·ªíNG CH√çNH C·ª¶A ·ª®NG D·ª§NG ---
local_css("style.css")

# --- PH·∫¶N LOGIC M·ªöI: QU·∫¢N L√ù TR·∫†NG TH√ÅI ---
if 'update_log' not in st.session_state:
    st.session_state.update_log = ""
if 'update_error' not in st.session_state:
    st.session_state.update_error = ""
if 'update_success' not in st.session_state:
    st.session_state.update_success = False

# Initialize session state for reading history and interest tracking
if 'reading_history' not in st.session_state:
    st.session_state.reading_history = []
if 'interest_vector' not in st.session_state:
    st.session_state.interest_vector = None
if 'interest_articles' not in st.session_state:
    st.session_state.interest_articles = None

def update_processing_progress(step, message, progress):
    """C·∫≠p nh·∫≠t ti·∫øn tr√¨nh x·ª≠ l√Ω."""
    st.session_state.processing_progress = {
        'step': step,
        'message': message,
        'progress': progress
    }

def append_update_log(message):
    if 'update_log' not in st.session_state:
        st.session_state.update_log = ""
    st.session_state.update_log += f"{message}\n"

def process_in_background():
    """X·ª≠ l√Ω d·ªØ li·ªáu trong n·ªÅn."""
    try:
        # Reset tr·∫°ng th√°i
        st.session_state.update_error = ""
        st.session_state.update_success = False
        
        # B∆∞·ªõc 1: T·∫£i b√†i vi·∫øt m·ªõi
        append_update_log("B·∫Øt ƒë·∫ßu c·∫≠p nh·∫≠t d·ªØ li·ªáu...")
        update_processing_progress('B·∫Øt ƒë·∫ßu', 'ƒêang t·∫£i b√†i vi·∫øt m·ªõi...', 10)
        df = fetch_recent_articles(RSS_URLS, hours=24)
        append_update_log(f"ƒê√£ l·∫•y {len(df)} b√†i vi·∫øt m·ªõi.")

        if df.empty:
            st.session_state.update_error = "Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt m·ªõi n√†o."
            append_update_log("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt m·ªõi n√†o.")
            return

        update_processing_progress('L√†m s·∫°ch', 'ƒêang x·ª≠ l√Ω vƒÉn b·∫£n...', 30)
        append_update_log("B·∫Øt ƒë·∫ßu l√†m s·∫°ch d·ªØ li·ªáu...")
        df = clean_text(df)
        append_update_log(f"Sau l√†m s·∫°ch c√≤n {len(df)} b√†i vi·∫øt.")

        if df.empty:
            st.session_state.update_error = "Kh√¥ng c√≥ b√†i vi·∫øt n√†o sau khi l√†m s·∫°ch d·ªØ li·ªáu."
            append_update_log("Kh√¥ng c√≥ b√†i vi·∫øt n√†o sau khi l√†m s·∫°ch d·ªØ li·ªáu.")
            return

        update_processing_progress('Vector h√≥a', 'ƒêang vector h√≥a n·ªôi dung...', 50)
        append_update_log("B·∫Øt ƒë·∫ßu vector h√≥a n·ªôi dung...")
        sbert_model = get_sbert_model()
        embeddings = vectorize_text(df['summary_not_stop_word'].tolist(), _model=sbert_model)
        append_update_log("ƒê√£ vector h√≥a xong.")

        update_processing_progress('Ph√¢n c·ª•m', 'ƒêang ph√¢n t√≠ch ch·ªß ƒë·ªÅ...', 70)
        append_update_log("B·∫Øt ƒë·∫ßu ph√¢n c·ª•m ch·ªß ƒë·ªÅ...")
        silhouette_scores = []
        possible_k_values = range(2, min(20, len(df) // 2))  # Gi·ªõi h·∫°n s·ªë c·ª•m d·ª±a tr√™n s·ªë l∆∞·ª£ng b√†i vi·∫øt
        for k in possible_k_values:
            kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans_temp.fit_predict(embeddings)
            score = silhouette_score(embeddings, labels)
            silhouette_scores.append(score)
        
        best_k = possible_k_values[np.argmax(silhouette_scores)]
        kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
        df['topic_cluster'] = kmeans.fit_predict(embeddings)
        append_update_log(f"ƒê√£ ph√¢n c·ª•m th√†nh {best_k} ch·ªß ƒë·ªÅ.")

        update_processing_progress('G√°n nh√£n', 'ƒêang t·∫°o nh√£n ch·ªß ƒë·ªÅ...', 85)
        append_update_log("B·∫Øt ƒë·∫ßu g√°n nh√£n ch·ªß ƒë·ªÅ...")
        topic_labels = get_topic_labels(df)
        append_update_log("ƒê√£ g√°n nh√£n ch·ªß ƒë·ªÅ.")

        update_processing_progress('Ho√†n t·∫•t', 'ƒêang t√≠nh to√°n ma tr·∫≠n t∆∞∆°ng ƒë·ªìng...', 95)
        append_update_log("B·∫Øt ƒë·∫ßu t√≠nh to√°n ma tr·∫≠n t∆∞∆°ng ƒë·ªìng...")
        cosine_sim = cosine_similarity(embeddings)
        append_update_log("ƒê√£ t√≠nh to√°n xong ma tr·∫≠n t∆∞∆°ng ƒë·ªìng.")

        st.session_state.processed_data = {
            'df': df,
            'cosine_sim': cosine_sim,
            'topic_labels': topic_labels,
            'embeddings': embeddings
        }
        update_processing_progress('Ho√†n t·∫•t', 'ƒê√£ c·∫≠p nh·∫≠t xong!', 100)
        st.session_state.update_success = True
        append_update_log("C·∫≠p nh·∫≠t ho√†n t·∫•t!")
    except Exception as e:
        st.session_state.update_error = f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}"
        append_update_log(f"L·ªói: {str(e)}")
    finally:
        st.session_state.is_processing = False

# Kh·ªüi t·∫°o d·ªØ li·ªáu ban ƒë·∫ßu
if 'processed_data' not in st.session_state:
    st.session_state.update_log = ""
    # L·∫ßn ƒë·∫ßu ch·∫°y, x·ª≠ l√Ω d·ªØ li·ªáu
    df, cosine_sim, topic_labels, embeddings = process_articles()
    if df is not None:
        st.session_state.processed_data = {
            'df': df,
            'cosine_sim': cosine_sim,
            'topic_labels': topic_labels,
            'embeddings': embeddings
        }
        st.write("### Nh·∫≠t k√Ω x·ª≠ l√Ω d·ªØ li·ªáu")
        st.code(st.session_state.update_log, language="log")
    else:
        df = pd.DataFrame()
        cosine_sim = None
        topic_labels = {}
        embeddings = None
else:
    # L·∫•y d·ªØ li·ªáu t·ª´ session state
    processed_data = st.session_state.processed_data
    df = processed_data.get('df', pd.DataFrame())
    cosine_sim = processed_data.get('cosine_sim', None)
    topic_labels = processed_data.get('topic_labels', {})
    embeddings = processed_data.get('embeddings', None)

sbert_model = get_sbert_model()

# --- GIAO DI·ªÜN THANH B√äN ---
st.sidebar.title("T·∫°p ch√≠ c·ªßa b·∫°n")
st.sidebar.markdown("---")

# C·∫≠p nh·∫≠t ph·∫ßn hi·ªÉn th·ªã ti·∫øn tr√¨nh trong sidebar
if st.sidebar.button("üîÑ C·∫≠p nh·∫≠t tin t·ª©c m·ªõi", use_container_width=True, disabled=st.session_state.is_processing):
    st.session_state.is_processing = True
    st.session_state.update_error = ""
    st.session_state.update_success = False
    process_in_background()

# Hi·ªÉn th·ªã ti·∫øn tr√¨nh x·ª≠ l√Ω
if st.session_state.is_processing:
    progress = st.session_state.processing_progress
    st.sidebar.progress(progress['progress'] / 100)
    st.sidebar.info(f"**{progress['step']}**: {progress['message']}")

# Hi·ªÉn th·ªã k·∫øt qu·∫£ c·∫≠p nh·∫≠t
if st.session_state.update_error:
    st.sidebar.error("‚ùå C·∫≠p nh·∫≠t th·∫•t b·∫°i!")
    with st.sidebar.expander("Xem chi ti·∫øt l·ªói"):
        st.code(st.session_state.update_error)
    if st.sidebar.button("Th·ª≠ l·∫°i", use_container_width=True, key="retry_button"):
        st.session_state.update_error = ""
        st.rerun()

if st.session_state.update_success:
    st.sidebar.success("‚úÖ C·∫≠p nh·∫≠t ho√†n t·∫•t!")
    if st.sidebar.button("Xem tin t·ª©c m·ªõi", use_container_width=True, key="view_new_button"):
        # C·∫≠p nh·∫≠t d·ªØ li·ªáu ch√≠nh
        if 'processed_data' in st.session_state:
            processed_data = st.session_state.processed_data
            df = processed_data.get('df', pd.DataFrame())
            cosine_sim = processed_data.get('cosine_sim', None)
            topic_labels = processed_data.get('topic_labels', {})
            embeddings = processed_data.get('embeddings', None)
        st.session_state.update_success = False
        st.rerun()

if st.session_state.is_processing or st.session_state.update_log:
    st.sidebar.markdown("#### Nh·∫≠t k√Ω c·∫≠p nh·∫≠t")
    st.sidebar.code(st.session_state.update_log, language="log")

st.sidebar.markdown("---")

# --- √î T√åM KI·∫æM SEMANTIC ·ªû HEADER ---
search_col1, search_col2 = st.columns([0.85, 0.15])
with search_col1:
    search_input = st.text_input(
        "T√¨m ki·∫øm b√†i vi·∫øt (theo ng·ªØ nghƒ©a, nh·∫≠p t·ª´ kh√≥a ho·∫∑c c√¢u h·ªèi):",
        value=st.session_state.get('search_query', ''),
        key="search_input",
        placeholder="Nh·∫≠p n·ªôi dung b·∫°n mu·ªën t√¨m...",
        label_visibility="collapsed"
    )
with search_col2:
    search_button = st.button("üîç T√¨m ki·∫øm", use_container_width=True, key="search_button")

if search_input and (search_button or search_input != st.session_state.get('search_query', '')):
    st.session_state['search_query'] = search_input
    st.session_state['current_view'] = "search"
    st.rerun()

if st.session_state.get('current_view', 'main') == "search" and st.session_state.get('search_query', ''):
    with st.spinner("ƒêang ph√¢n t√≠ch v√† t√¨m ki·∫øm..."):
        query_vector = sbert_model.encode([st.session_state['search_query']])
        similarities = cosine_similarity(query_vector, embeddings)[0]
        sim_scores = sorted(list(enumerate(similarities)), key=lambda x: x[1], reverse=True)
        result_indices = [i[0] for i in sim_scores]
        result_df = df.iloc[result_indices].copy()
    st.sidebar.info("B·∫°n ƒëang ·ªü trang k·∫øt qu·∫£ t√¨m ki·∫øm. Ch·ªçn danh m·ª•c kh√°c ho·∫∑c b·∫•m 'Quay l·∫°i' ƒë·ªÉ tr·ªü v·ªÅ.")
    if st.sidebar.button("‚¨ÖÔ∏è Quay l·∫°i trang ch·ªß", use_container_width=True, key="back_to_main"):
        st.session_state['search_query'] = ''
        st.session_state['current_view'] = "main"
        st.rerun()
    render_main_grid(result_df, f"K·∫øt qu·∫£ cho: \"{st.session_state['search_query']}\"")
    st.stop()
elif st.session_state.current_view == "detail" and st.session_state.current_article_id is not None:
    render_detail_view(st.session_state.current_article_id, df, cosine_sim, topic_labels)
else:
    if df is None:
        st.error("L·ªói: Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu. Vui l√≤ng b·∫•m n√∫t 'C·∫≠p nh·∫≠t tin t·ª©c m·ªõi' ·ªü thanh b√™n.")
    else:
        # --- PH·∫¶N L·ªåC THEO CH·ª¶ ƒê·ªÄ ---
        st.sidebar.subheader("Kh√°m ph√° c√°c ch·ªß ƒë·ªÅ")
        topic_display_list = ["D√†nh cho b·∫°n (T·∫•t c·∫£)", "B√†i vi·∫øt ƒë√£ ƒë·ªçc", "D·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc"] + [f"{v} ({k})" for k, v in topic_labels.items()]
        
        st.sidebar.markdown('<div class="sidebar-nav">', unsafe_allow_html=True)
        for topic in topic_display_list:
            is_active = (topic == st.session_state.selected_topic)
            active_class = "active" if is_active else ""
            icon = "üìñ" if topic != "B√†i vi·∫øt ƒë√£ ƒë·ªçc" and topic != "D·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc" else "üëÅÔ∏è" if topic == "B√†i vi·∫øt ƒë√£ ƒë·ªçc" else "üéØ"
            if st.sidebar.button(f"{icon} {topic}", key=f"topic_{topic.replace(' ', '_')}", use_container_width=True):
                st.session_state.selected_topic = topic
                st.rerun()
        st.sidebar.markdown('</div>', unsafe_allow_html=True)
        st.sidebar.markdown("---")

        # --- B·ªî SUNG: PH·∫¶N L·ªåC THEO NGU·ªíN ---
        st.sidebar.subheader("L·ªçc theo ngu·ªìn")
        all_sources = sorted(df['source_name'].unique().tolist())
        selected_sources = st.sidebar.multiselect(
            "Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu ngu·ªìn:",
            options=all_sources,
            default=st.session_state.selected_sources
        )
        
        if selected_sources != st.session_state.selected_sources:
            st.session_state.selected_sources = selected_sources
            st.rerun()

        # --- HI·ªÇN TH·ªä VIEW T∆Ø∆†NG ·ª®NG ---
        if st.session_state.selected_topic == "B√†i vi·∫øt ƒë√£ ƒë·ªçc":
            if st.session_state.read_articles:
                # L·∫•y danh s√°ch b√†i vi·∫øt ƒë√£ ƒë·ªçc theo th·ª© t·ª± trong reading_history (m·ªõi nh·∫•t l√™n ƒë·∫ßu)
                ordered_articles = [article_id for article_id in st.session_state.reading_history if article_id in st.session_state.read_articles]
                # T·∫°o DataFrame v·ªõi th·ª© t·ª± ƒë√£ s·∫Øp x·∫øp
                display_df = df[df.index.isin(ordered_articles)].copy()
                # S·∫Øp x·∫øp l·∫°i theo th·ª© t·ª± trong ordered_articles
                display_df = display_df.reindex(ordered_articles)
            else:
                display_df = pd.DataFrame()
                st.info("B·∫°n ch∆∞a ƒë·ªçc b√†i vi·∫øt n√†o.")
        elif st.session_state.selected_topic == "D·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc":
            if len(st.session_state.reading_history) > 0:
                display_df = get_similar_articles_by_history(
                    df, cosine_sim,
                    st.session_state.reading_history,
                    exclude_articles=st.session_state.read_articles
                )
                if display_df.empty:
                    st.info("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt t∆∞∆°ng t·ª± d·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc.")
            else:
                display_df = pd.DataFrame()
                st.info("B·∫°n ch∆∞a c√≥ l·ªãch s·ª≠ ƒë·ªçc b√†i vi·∫øt n√†o.")
        elif st.session_state.selected_topic != "D√†nh cho b·∫°n (T·∫•t c·∫£)":
            # T√°ch cluster ID t·ª´ t√™n ch·ªß ƒë·ªÅ (format: "T√™n ch·ªß ƒë·ªÅ (cluster_id)")
            topic_parts = st.session_state.selected_topic.split(" (")
            if len(topic_parts) > 1:
                try:
                    cluster_id = int(topic_parts[1].rstrip(")"))
                    display_df = df[df['topic_cluster'] == cluster_id].copy()
                except ValueError:
                    # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát nh∆∞ "B√†i vi·∫øt ƒë√£ ƒë·ªçc" v√† "D·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc"
                    if st.session_state.selected_topic == "B√†i vi·∫øt ƒë√£ ƒë·ªçc":
                        # ... x·ª≠ l√Ω b√†i vi·∫øt ƒë√£ ƒë·ªçc ...
                        display_df = df[df['topic_cluster'] == cluster_id].copy()
                    elif st.session_state.selected_topic == "D·ª±a tr√™n l·ªãch s·ª≠ ƒë·ªçc":
                        # ... x·ª≠ l√Ω b√†i vi·∫øt d·ª±a tr√™n l·ªãch s·ª≠ ...
                        display_df = pd.DataFrame()
                    else:
                        display_df = pd.DataFrame()
            else:
                display_df = pd.DataFrame()
        else:
            display_df = df.copy()

        # √Åp d·ª•ng b·ªô l·ªçc ngu·ªìn
        if st.session_state.selected_sources:
            display_df = display_df[display_df['source_name'].isin(st.session_state.selected_sources)]

        # S·∫Øp x·∫øp v√† hi·ªÉn th·ªã
        if not display_df.empty:
            # Ch·ªâ s·∫Øp x·∫øp theo th·ªùi gian ƒëƒÉng n·∫øu kh√¥ng ph·∫£i l√† b√†i vi·∫øt ƒë√£ ƒë·ªçc
            if st.session_state.selected_topic != "B√†i vi·∫øt ƒë√£ ƒë·ªçc":
                display_df = display_df.sort_values(by='published_time', ascending=False)
        render_main_grid(display_df, st.session_state.selected_topic)
